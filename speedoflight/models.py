from datetime import datetime
from enum import Enum
from typing import Annotated, Any, Literal, Optional, Union

from gi.repository import GObject  # type: ignore
from pydantic import BaseModel, Field

from speedoflight.utils import generate_uuid, get_now_utc

#
# Configuration
#


class LLMProvider(Enum):
    OLLAMA = "ollama"
    ANTHROPIC = "anthropic"


class BaseLLMConfig(BaseModel):
    temperature: float = 0.25
    model: str


class OllamaConfig(BaseLLMConfig):
    type: Literal["ollama"] = "ollama"

    # Only models that support tools should be used, some examples
    # from https://ollama.com/search?c=tools:
    # "mistral-small:latest",  # 14 GB
    # "devstral:latest",  # 14 GB
    # "mistral-nemo:latest",  # 7.1 GB
    # "qwen3:latest",  # 5.2 GB
    # "mistral:latest",  # 4.4 GB
    # "phi4-mini:latest",  # 2.5 GB
    # "llama3.2:latest",  # 2.0 GB
    model: str = "mistral-small:latest"
    host: str = "http://localhost:11434"


class AnthropicConfig(BaseLLMConfig):
    type: Literal["anthropic"] = "anthropic"

    # E.g. "claude-sonnet-4-0" or "claude-opus-4-0"
    model: str = "claude-sonnet-4-0"
    max_tokens: int = 8192
    api_key: str
    enable_web_search: bool = False


class BaseMCPConfig(BaseModel):
    enabled: bool = True
    enabled_tools: list[str] = []


class StdioConfig(BaseMCPConfig):
    type: Literal["stdio"] = "stdio"
    command: str
    args: list[str] = []
    env: Optional[dict[str, str]] = None


class StreamableHttpConfig(BaseMCPConfig):
    type: Literal["streamable_http"] = "streamable_http"
    url: str
    headers: Optional[dict[str, str]] = None


LLMConfig = Annotated[Union[OllamaConfig, AnthropicConfig], Field(discriminator="type")]

MCPConfig = Annotated[
    Union[StdioConfig, StreamableHttpConfig], Field(discriminator="type")
]


class AppConfig(BaseModel):
    llm: LLMProvider = LLMProvider.OLLAMA
    llms: Optional[dict[str, LLMConfig]] = None
    mcps: Optional[dict[str, MCPConfig]] = None
    max_iterations: int = 10


#
# Messages
#


class BaseBlock(BaseModel):
    id: str = Field(default_factory=generate_uuid)
    created_at: datetime = Field(default_factory=get_now_utc)


class TextBlockRequest(BaseBlock):
    text: str


class ImageBlockRequest(BaseBlock):
    encoded: str


class TextBlockResponse(BaseBlock):
    text: str
    text_html: Optional[str] = None


class ImageBlockResponse(BaseBlock):
    encoded: str


class ToolEnvironment(Enum):
    LOCAL = "local"
    SERVER = "server"


class ToolInputResponse(BaseBlock):
    call_id: str
    environment: ToolEnvironment
    name: str
    arguments: dict


class ToolTextOutputRequest(BaseBlock):
    """Represents tool output from a local execution (e.g. MCP) that needs
    to be added to the next LLM request."""

    call_id: str
    name: str
    text: str
    is_error: bool


class ToolTextOutputResponse(BaseBlock):
    """Represents tool output from a server execution (e.g. web search) that
    does not need to be added to the next LLM request."""

    call_id: str
    name: str
    text: str
    is_error: bool


class ImageMimeType(Enum):
    JPEG = "image/jpeg"
    PNG = "image/png"
    GIF = "image/gif"
    WEBP = "image/webp"


class ToolImageOutputRequest(BaseBlock):
    call_id: str
    name: str
    data: str  # The base64-encoded image data.
    mime_type: ImageMimeType
    is_error: bool


class MessageRole(Enum):
    """Represents the source of the message."""

    SOL = "sol"
    HUMAN = "human"
    AI = "ai"
    TOOL = "tool"


class StopReason(Enum):
    END_TURN = "end_turn"
    MAX_TOKENS = "max_tokens"
    STOP_SEQUENCE = "stop_sequence"
    TOOL_USE = "tool_use"
    PAUSE_TURN = "pause_turn"
    REFUSAL = "refusal"


class Usage(BaseModel):
    input_tokens: Optional[int] = None
    output_tokens: Optional[int] = None


class BaseMessage(BaseModel):
    id: str = Field(default_factory=generate_uuid)
    created_at: datetime = Field(default_factory=get_now_utc)
    role: MessageRole


class SolMessage(BaseMessage):
    message: str


class RequestMessage(BaseMessage):
    """Represents content that is provided to the LLM."""

    content: list[
        TextBlockRequest
        | ImageBlockRequest
        | ToolTextOutputRequest
        | ToolImageOutputRequest
    ]


class ResponseMessage(BaseMessage):
    """Represents content that is generated by the LLM."""

    # The raw response from the LLM. Excluded because there are no guarantees
    # that all SDKs make their model classes easily serializable.
    raw: Optional[Any] = Field(default=None, exclude=True)

    provider: Optional[str] = None
    model: Optional[str] = None
    usage: Optional[Usage] = None
    stop_reason: Optional[StopReason] = None
    stop_sequence: Optional[str] = None
    content: list[
        TextBlockResponse
        | ImageBlockResponse
        | ToolInputResponse
        | ToolTextOutputResponse
    ]


class GBaseMessage(GObject.Object):
    def __init__(self, data: BaseMessage):
        super().__init__()
        self.data = data


#
# Agent
#


class AgentRequest(BaseModel):
    session_id: str
    message: RequestMessage


class AgentResponse(BaseModel):
    is_error: bool
    message: Optional[SolMessage] = None
